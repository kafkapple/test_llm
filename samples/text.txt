llm debug
lora?
threshold 설정:
높을수록: 더 많은 가중치가 원본 precision 유지 (정확도 ↑)
낮을수록: 더 많은 가중치가 양자화 (메모리 효율 ↑

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.



fp16: true → fp16: false로 변경 (FP16 비활성화)
bf16: false → bf16: true로 변경 (BF16 활성화)
이 변경으로 FP16 그래디언트 스케일링 오류를 해결할 수 있습니다. BF16은 FP16보다 더 안정적이며, 특히 그래디언트 스케일링 문제가 덜 발생합니다.

device_map="auto" 설정을 사용할 때는 to_device가 필요 없습니다.
그 이유는:
device_map="auto"를 사용하면 Transformers 라이브러리가 자동으로:
사용 가능한 GPU 메모리를 확인
모델을 적절한 디바이스에 배치
필요한 경우 모델을 여러 GPU에 분산

temperature 로 logit  나눔

temp 클 수록, logit 전체적으로 낮아져, 확률값들 차이가 낮아짐. 원래 낮은 애들도 뽑히기 좋아짐.
반대로 낮으면, 확률값 구분 선명해져, 뽑힐 애들만 뽑히게 됨.



RTX 3060의 VRAM이 12GB인데 8B 모델을 로드하려고 하니 메모리가 부족한

4비트 양자화를 사용하여 모델 크기를 대폭 감소 (8비트 대비 2배 감소)
이중 양자화로 추가 메모리 절약
float16 precision 사용
GPU 메모리 제한을 10GB로 설정 (RTX 3060 VRAM 고려)
